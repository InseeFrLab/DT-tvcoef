---
title: "Utilisation de modèles de régression à coefficients variant dans le temps pour la prévision conjoncturelle"
format: 
  pdf:
    include-in-header: template/preambule.tex
    keep-tex: true
    classoption: french
    cite-method: biblatex
    pdf-engine: pdflatex
    papersize: A4
    fig-width: 7.5
  html: 
    self-contained: false
    include-before-body: template/preambule.html
    css: [template/style.css]
    code-links:
      - text: InseeFrLab/DT-tvcoef
        icon: github
        href: https://github.com/AQLT/DT-tvcoef
    crossref: 
      title-delim: "-"
    output-file: index
  # docx: default
execute: 
  cache: true
toc: true
number-sections: true
bibliography: biblio.bib
biblio-style: authoryear
crossref:
  tbl-prefix: table
  fig-prefix: figure
  sec-prefix: section
  eq-prefix: équation
tbl-cap-location: top
fig-cap-location: top
author:
  - name: Alain Quartier-la-Tente 
    affiliation: Insee
    affiliation-url: https://www.insee.fr/fr/
citation:  
  type: article-journal
  container-title: "Document de travail méthodologique Insee"
  issued: 2024
  url: https://github.com/InseeFrLab/DT-tvcoef
  pdf-url: https://www.insee.fr/fr/statistiques/7759578
  # key: inseeDTM2024XX
comments: 
    hypothesis:
      theme: clean
      # openSidebar: false
# license:
#   text: > 
#     Copyright (c) 2024 Alain Quartier-la-Tente, INSEE
# 
#     Permission is hereby granted, free of charge, to any person obtaining a copy
#     of this software and associated documentation files (the "Software"), to deal
#     in the Software without restriction, including without limitation the rights
#     to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#     copies of the Software, and to permit persons to whom the Software is
#     furnished to do so, subject to the following conditions:
#     
#     The above copyright notice and this permission notice shall be included in all
#     copies or substantial portions of the Software.
#     
#     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#     SOFTWARE.
#   type: MIT
lang: fr
language:
 title-block-author-single: Auteur
---

```{r}
#| include: false
#| cache: false
library(tvCoef)
library(dynlm)
library(strucchange)
library(tvReg)
library(ggplot2)
library(patchwork)
library(forecast)
library(gt)
library(ggforce)

# # colors = pal_viridis()(4),
# # colors = rev(pal_brewer("qual")(4)),
# colors = pal_hue()(4)
colors_graph <- c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")
coef2plot <- function(x, linetype = 1, start = NULL, end = NULL,
                      colors = colors_graph){
  noms_mod <- names(x)
  linetype <- rep(linetype, length(x))
  x <- lapply(x, window, start = start, end = end, extend =TRUE)
  plot_i <- function(x, col){
    p <- autoplot(x[[1]][,col], series = noms_mod[1], linetype=linetype[1],size = 1.2) + 
      labs(y = NULL, x = NULL, title = colnames(x[[1]])[col])
    
    for (i in seq_len(length(x)-1)+1) {
      p <- p + autolayer(x[[i]][,col], series = noms_mod[i], linetype=linetype[i], size = 1.2)
    }
    p +
      theme_bw() + 
      theme(legend.title=element_blank())+
      scale_color_manual(breaks = noms_mod,
                             values = colors)
  }
  (plot_i(x, 1) + plot_i(x, 2) / plot_i(x, 3)) + 
    plot_layout(guides = 'collect') 
}
```


<!-- \thispagestyle{fancy} -->

::: {.content-visible when-format="html"}
Remerciements :
:::

\newpage

# Résumé {-}

:::: {.abstract}
Cette étude décrit trois méthodes d'estimation de modèles de régression linéaire avec des coefficients variant dans le temps : régression par morceaux, régression locale et régression avec coefficients stochastiques (modélisation espace-état).
Elle détaille également leur implémentation sous R grâce au package `tvCoef`.
À travers une analyse comparative sur une trentaine de modèles de prévision trimestrielle, nous montrons que l'utilisation de ces méthodes, notamment par la modélisation espace-état, réduit les erreurs de prévision lorsque des ruptures sont présentes dans les coefficients.
Par ailleurs, même lorsque les tests classiques concluent à la constance des coefficients, la régression avec coefficients stochastiques peut permettre de réduire les erreurs de prévision.
Cependant, les incertitudes liées à l'estimation de certains hyperparamètres peuvent augmenter les erreurs de prévision en temps réel, en particulier pour la régression locale. 
Ainsi, une analyse économique des paramètres estimés demeure essentielle.

Cette étude est entièrement reproductible et tous les codes utilisés sont disponibles sous <https://github.com/InseeFrLab/DT-tvcoef>.

Mots clés : séries temporelles, prévisions, séries longues.
::::


# Abstract {-}

:::: {.abstract}
This study describes three methods for estimating linear regression models with time-varying coefficients: piecewise regression, local regression, and regression with stochastic coefficients (state-space modeling).
It also details their implementation in R using the `tvCoef` package.
Through a comparative analysis of around thirty quarterly forecasting models, we show that the use of these methods, especially thanks to the state-space modeling, reduces forecast errors when breakpoints are present in the coefficients.
Moreover, even when traditional tests conclude stability of coefficients, regression with stochastic coefficients can still improve forecasts.
However, uncertainties related to estimating certain hyperparameters can increase real-time forecast errors, especially for local regression.
Thus, an economic analysis of estimated parameters remains essential.

This study is fully reproducible and all the codes used are available under <https://github.com/InseeFrLab/DT-tvcoef>.

Keywords: time series, forecast, long time series.
::::

JEL Classification: C22, C53.

\newpage


# Introduction

Dans la statistique publique, de nombreux modèles de prévision s'appuient sur des régressions linéaires. 
Par exemple, les producteurs de séries désaisonnalisées appliquent des modèles RegARIMA pour la correction des effets de calendrier et les comptes nationaux trimestriels utilisent des modèles d'étalonnage-calage pour caler les séries sur les comptes nationaux annuels. 
<!-- nationaux ? pour un lecteur lambda, lire "comptes" ne va pas lui dire grand-chose -->
<!-- Rep AQLT : Il manque la fin de la phrase, est-ce que tu crois que je rajoute des références pour chaque exemple comme pour la phrase d'après ? -->
Pour la prévision des grands agrégats macroéconomiques, l'Insee [e.g., @ndc2015prev] et la Banque de France [e.g., @OPTIM] utilisent notamment des modèles de régression linéaire pour prévoir la croissance et le modèle macroéconomique Mésange [@mesange] s'appuie sur des modèles à correction d'erreur pour modéliser les comportements macroéconomiques. 
Ces méthodes fournissent généralement de bons résultats et ont l'avantage d'être facilement interprétables. 
Cependant, elles supposent que les relations entre les variables (i.e, les coefficients estimés) sont fixes dans le temps : cette hypothèse peut avoir du sens sur courte période mais n'est généralement plus vérifiée lorsque les modèles sont estimés sur longue période, ce qui conduit à des modèles sous-optimaux.

Pour palier à ce problème, une solution simple consiste à utiliser moins de données pour estimer les modèles.
Par exemple, le guide des bonnes pratiques sur l'ajustement saisonnier [@eurostat2015guidelines] recommande de ne pas désaisonnaliser des séries de plus de 20 ans.
Toutefois, cela conduit à perdre l'historique des données et l'information que l'on peut en tirer et ne résout pas le problème lorsque la rupture est récente.
Par ailleurs, comme montré par @JMS2018 pour la désaisonnalisation des séries d'indice de production industrielle, lorsqu'il faut analyser les modèles sur l'ensemble de la période (par exemple dans le cadre de la correction des jours ouvrables), il est nécessaire de mettre en place des méthodes de chaînage afin de prendre en compte la rupture introduite par l'utilisation de plusieurs modèles.
Ainsi, dans certains cas il peut être préférable d'utiliser des modèles qui prennent directement en compte les ruptures.

Cette étude s'intéresse à différentes méthodes d'estimation de coefficients variant dans le temps dans le cadre de la prévision conjoncturelle. 
Cela permet d'avoir des modèles qui ont l'avantage d'être plus facilement interprétables que des méthodes de machine learning, puisqu'ils s'appuient sur des modèles de régression linéaire.
Ces méthodes se regroupent en trois catégories : les modèles de régression par morceaux, les régressions locales et les régressions avec coefficients stochastiques (estimés par une modélisation espace-état).
La première suppose l'existence d'une rupture brutale sur les coefficients à une certaine date ; les deux autres supposent que les coefficients évoluent progressivement sans le temps sans existence de rupture brutale.
Pour simplifier l'implémentation de ces méthodes, ainsi que leur comparaison, le package R `tvCoef` (<https://github.com/InseeFrLab/tvCoef>) a également été développé lors de cette étude. 
Cette étude est entièrement reproductible et tous les codes utilisés sont disponibles sous <https://github.com/InseeFrLab/DT-tvcoef>.

Après une description de deux tests permettant de tester si les coefficients sont fixes dans le temps (@sec-tests), nous décrivons trois méthodes pour estimer des coefficients variant dans le temps et montrons comment les implémenter à partir d'un modèle de prévision de la croissance du PIB français (@sec-desc-meth).
<!-- permettant de vérifier si les coefficients sont fixes dans le temps: formulation peut-etre un peu trop forte. à la fin, on n’a que le résultat du test, pas la vérification définitive que les coefficients sont fixes ou non. les résultats des tests peuvent être flous -->
<!-- Rep AQLT : c'était pour éviter la formulation "tests permettant de tester" mais c'est finalement la plus simple -->
Nous comparons ensuite les qualités prédictives des différentes méthodes sur une trentaine de modèles de prévision trimestrielle (@sec-comp-generales).
Nous montrons que, lorsque l'hypothèse de constance des coefficients n'est pas vérifiée, l'utilisation de ces modèles (notamment la régression avec coefficients stochastiques) permet de réduire les erreurs de prévision. 
Par ailleurs, même lorsque les tests classiques concluent à la constance des coefficients, la régression avec coefficients stochastiques peut permettre de réduire les erreurs de prévision.



# Modélisation générale et tests {#sec-tests}

Dans cette étude, nous nous plaçons dans le cadre de la régression linéaire avec des variables à une dimension.
À chaque date $t$, la variable $y_t$ (e.g., taux de croissance du PIB) est expliquée par une combinaison linéaire d'une constante et de $p$ variables explicatives, $x_{1,t},\dots,x_{p,t}$ (soldes d'opinion, indices de production industrielle, indicatrices, etc.) :
<!-- si tu commences à x_0, ça fait p+1 variables. -->
<!-- Rep AQLT : c'était en fait pour différencier la constante des autres variables, j'ai reformulé. Mais si tu penses que ce n'est pas clair je dis p+1 variables en commençant à x_{0,t} et en indiquant que x_{0,t} = 1 -->
$$
y_t=\alpha_{0}+\alpha_{1} x_{1,t}+\dots+\alpha_{p} x_{p,t} +\varepsilon_t 
$$
où $\varepsilon_t$ représente le terme d'erreur.
<!-- erreur d’approximation? je ne connais pas ce terme. ce n’est pas juste le terme d’erreur? -->
<!-- Rep AQLT : effectivement juste terme d'erreur c'est mieux -->
En notant ${\bf X}_t=\begin{pmatrix}1 & x_{1,t} &\cdots & x_{p,t} \end{pmatrix}$ et 
${\bf \alpha}=\transp{\begin{pmatrix}\alpha_0 & \alpha_1 &\cdots & \alpha_p \end{pmatrix}}$, cela s'écrit matriciellement\ :
$$
y_t={\bf X_t} \bf\alpha +\varepsilon_t.
$$ {#eq-mod-fixe}

Dans le cadre de la régression linéaire, les coefficients $\bf\alpha$ sont supposés constants dans le temps et estimés en utilisant l'ensemble des données.
Cela suppose donc que la relation économique entre les différentes variables est stable dans le temps.
Même si cette hypothèse est généralement vraie sur le court-terme, elle peut être invalidée sur le long-terme du fait de changements structurels (mesures économiques, crises, changement de nomenclature, etc.).
L'objectif de cette étude est d'étudier différents modèles permettant de relâcher cette hypothèse de constance des coefficients.
Le modèle général s'écrit donc :
$$
y_t={\bf X_t} \bf\alpha_t  +\varepsilon_t.
$$
Pour faciliter l'utilisation des modèles ici présentés, le package {{< fa brands r-project >}} `tvCoef` [@tvcoef] a été développé.
Leur implémentation est illustrée à travers l'exemple de la prévision du taux de croissance trimestriel du PIB, noté $y_t$, à partir du climat des affaires France publié par l'Insee^[
Cette série est disponible à l'URL <https://www.insee.fr/fr/statistiques/serie/001565530>.
].
Ces séries sont disponibles sous {{< fa brands r-project >}} dans la base de donnée `tvCoef::gdp`^[
Les données sont disponibles dans le package `tvCoef` ont été téléchargées le 15 mars 2024 et peuvent donc différer de celles actuellement disponibles.
] :

- `growth_gdp` correspond au taux de croissance trimestriel du PIB ;

- `bc_fr_m1` correspond au climat des affaires au premier mois de chaque trimestre (la valeur de 2000T1 correspond à la valeur de janvier 2000, celle de 2000T2 à celle d'avril 2000, etc.) ;

- `diff_bc_fr_m1` correspond à la différenciation trimestrielle de la variable précédente (la valeur de 2000T1 correspond à la différence du climat des affaires entre janvier 2000 et octobre 1999).

Les graphiques de ces variables sont disponibles dans l'annexe [-@sec-an-graph].

Le modèle s'écrit donc :
$$
y_t=\alpha_0 + \alpha_1\times climat\_fr_t^{m_1} + \alpha_2\times \Delta climat\_fr_t^{m_1}+\varepsilon_t.
$$
<!-- la notation %PIB est un peu suprenante. tu tiens à la conserver? tu peux utiliser y_t plus simplement ? si c’est delta(log(PIB)), alors \Delta(y), et y = log(PIB). -->
<!-- Rep AQLT : c'est bien l'évolution et pas le delta log, je mets donc plutôt y_t partout mais je trouve ça moins direct que l'on prend le taux de croissance du PIB -->
Il est estimé en utilisant les données entre les années 1980 et 2019.
Sous {{< fa brands r-project >}}, ce modèle peut être estimé en utilisant la fonction `stats::lm()`.
Toutefois, nous recommandons d'utiliser le package `dynlm` [@dynlm] qui offre une plus grande flexibilité dans la définition des modèles et permet de conserver le format série temporelle dans les fonctions de `tvCoef`.

```{r}
#| warning: false
#| message: false
library(tvCoef)
library(dynlm)
data_gdp <- window(gdp, start = 1980, end = c(2019, 4))
reg_lin <- dynlm(
  formula = growth_gdp ~ bc_fr_m1 + diff_bc_fr_m1,
  data = data_gdp
)
# # Equivalent à :
# reg_lin <- dynlm(
#   formula = growth_gdp ~ bc_fr_m1 + diff(bc_fr_m1, 1),
#   # Date de début changée car on perd une donnée avec la différenciation
#   data = window(gdp, start = c(1979, 4), end = c(2019, 4))
# )
summary(reg_lin)
```
<!-- tu peux donner les écarts types? (plus généralement, la table usuelle de régression? On sera contents d’avoir les différents R2 à comparer. et puis de se rendre compte si les écarts types peuvent aider à se rendre compte qu’il y a peut-être un souci -- ou pas justement !) -->
<!-- Rep AQLT : J'ai changé le coefficients() en summary, l'output est donc plus gros. concernant le R2 je ne préfère pas insister dessus car ce n'est généralement pas recommandé pour comparer les modèles de prévision -->
Le modèle estimé est donc :
```{r}
#| echo: false
#| output: asis
cat(sprintf("$$y_t=%.2f + %.2f\\times climat\\_fr_t^{m_1} + %.2f\\times \\Delta climat\\_fr_t^{m_1}+{\\hat\\varepsilon}_t.$$",
            coef(reg_lin)[1],
            coef(reg_lin)[2],
            coef(reg_lin)[3]))
```



## Test de rupture brutale {#sec-test-baiperron}

L'idée la plus simple pour tester s'il y a une rupture dans l'estimation des coefficients à une date $t_1$, est d'estimer deux sous-modèles avant et après cette date :
$$
\begin{cases}
\forall t \leq t_1 :\quad y_t = \alpha_0' + \alpha_1' climat\_fr_t + \alpha_2' \Delta climat\_fr_t + \varepsilon_t' \\
\forall t > t_1 :\quad y_t = \alpha_0'' + \alpha_1'' climat\_fr_t + \alpha_2'' \Delta climat\_fr_t + \varepsilon_t''
\end{cases}.
$$
Il ne reste ensuite qu'à tester si les coefficients estimés entre les deux sous-périodes sont égaux\ : $\alpha_0' = \alpha_0''$, $\alpha_1' = \alpha_1''$ et $\alpha_2' = \alpha_2''.$
L'hypothèse alternative est qu'au moins un des coefficients est différent entre les deux sous-périodes.
C'est le principe du test de @chowtest.
<!-- j’ai besoin d’un rappel: le test de chow teste vraiment l’égalité des coefs entre 2 régressions différentes? ou alors on fait une seule régression avec l’ensemble des points, et 2 coefficients différents selon la sous-période? c’est pas loin d’être la même chose, mais pas tout à fait (les erreurs). et comparer les coefs de 2 régressions différentes fait peut-être intervenir des hypothèses supplémentaires? -->
<!-- Rep AQLT : dans le test de chow tu fais trois régressions : les deux sous-périodes + estimation sur l'ensemble et ensuite un test de Fisher est fait. -->
<!--   Je pense que derrière il y a l'hypothèse que les variances sont égales (puisque c'est une hypothèse pour l'estimation du modèle sur l'ensemble de la période) mais cela n'est pas contraint dans l'estimation des deux sous-modèles. -->

L'inconvénient est que cela suppose d'avoir un *a priori* sur la date de la rupture à tester.
Pour palier à ce problème, @bai2003computation ont proposé un algorithme efficace afin de chercher la présence de ruptures multiples dans des modèles de régression linéaire.
Cet algorithme a été implémenté sous {{< fa brands r-project >}} dans le package `strucchange` [@strucchangeBP].
La fonction `strucchange::breakpoints()` permet de chercher les ruptures et la fonction `strucchange::breakdates()` permet d'extraire facilement les dates associées.
Le package `tvCoef` implémente une méthode `breakpoints.lm()` afin de pouvoir directement appliquer cette fonction aux régressions linéaires estimées :

<!-- pas très clair pour moi si breakpoints, breakdates et confint que tu utilises dans les chunk sont des fonctions de strucchange ou de tvCoef. peut-etre que tu veux expliciter ça pour le lecteur qui veut ensuite utiliser ces fonctions -->
<!-- Rep AQLT : dans le texte, lorsque je mentionne les fonctions j'essaye de mettre le package avant, exemple strucchange::breakdates() dans le paragraphe précédent (mais je vois que je ne l'avais pas fait pour confint qui est dans stats). -->
<!--   Je n'ai pas fait ça dans le code car ça peut l'alourdir (et que dans ce cas le chargement du package est inutile) mais si tu penses que ça serait plus clair je le rajoute également dans le code -->
```{r}
library(strucchange)
bp <- breakpoints(reg_lin)
breakdates(bp)
```
Une seule rupture est détectée au `{r} format(zoo::as.yearqtr(strucchange::breakdates(bp)), "%YT%q")`.
Un intervalle de confiance autour de la date détectée peut être calculée en utilisant la fonction `stats::confint()` :
```{r}
breakdates(confint(bp))
```
L'incertitude autour de la date détectée est grande ! 
Il y a 95 % de chance que la rupture soit comprise entre `{r} format(zoo::as.yearqtr(strucchange::breakdates(stats::confint(bp))[1]), "%YT%q")` et `{r} format(zoo::as.yearqtr(strucchange::breakdates(confint(bp))[3]), "%YT%q")`.

Cet algorithme est très simple à utiliser mais possède plusieurs inconvénients :

- L'implémentation sous {{< fa brands r-project >}} de l'algorithme de Bai et Perron ne permet pas de chercher des ruptures sur un sous-ensemble de variables : on ne cherche des ruptures que sur l'ensemble du modèle.
Par exemple, on ne peut pas tester $\alpha_2' = \alpha_2''$ dans le modèle :
$$
\begin{cases}
\forall t \leq t_1 :\quad y_t = \alpha_0 + \alpha_1 climat\_fr_t + \alpha_2' \Delta climat\_fr_t + \varepsilon_t' \\
\forall t > t_1 :\quad y_t = \alpha_0 + \alpha_1 climat\_fr_t + \alpha_2'' \Delta climat\_fr_t + \varepsilon_t''
\end{cases}.
$$
<!-- Une solution simple est d'effectuer une première régression sur l'ensemble des données afin d'estimer $\alpha_0$ et $\alpha_1$ et d'ensuite appliquer la procédure de Bai et Perron sur le modèle\ : -->
<!-- <!-- pas sûr que ce soit une solution si simple. si on suit Frisch-Waugh, il faudrait plutôt, pour ne pas avoir de alpha_0 et alpha_1 biaisés par le biais de variable omise (lié à la variable Delta Climat), régresser variables de gauche, la constante et climat sur Delta(climat) et ensuite régresser les résidus de y sur résidu de constante et résidu de climat. là ça donne alpha_0 et alpha_1. puis tu fais ce que tu proposes. non?  --> -->
<!-- <!-- Rep AQLT : à rediscuter mais est-ce que le biais de variable omise affecte tous les coefficients ? Même si le coefficient est biaisé je ne pense pas que cela biaise l'estimation de la date de rupture. Une fois la date de rupture détectée, ce que je fais c'est estimer une nouvelle régression avec toutes les variables + celles coupées par morceaux. --> -->
<!-- $$ -->
<!-- \begin{cases} -->
<!-- \forall t \leq t_1 :\quad (y - \hat\alpha_0-\hat \alpha_1 climat\_fr)_t = \alpha_2' \Delta climat\_fr_t + \varepsilon_t' \\ -->
<!-- \forall t > t_1 :\quad (y - \hat\alpha_0-\hat \alpha_1 climat\_fr)_t = \alpha_2'' \Delta climat\_fr_t + \varepsilon_t'' -->
<!-- \end{cases}. -->
<!-- $$ -->

- Il y a une instabilité sur le choix de la date et il suppose que la rupture est brutale à une certaine date.
Si la rupture est brutale, le statisticien doit pouvoir expliquer son origine (changement de nomenclature, de champ dans les données, crise...) et a déjà un *a priori* sur la date de rupture.
Si l'on n'a aucune information sur la présence d'une rupture, on peut raisonnablement penser que celle-ci n'est pas brutale mais que la relation entre les variables a évolué de manière progressive dans le temps.


## Test de constance des coefficients {#sec-hansen-test}

Alors que l'algorithme de Bai et Perron cherche une date spécifique où il y aurait une rupture dans les modèles,
@hansen1992testing propose une procédure permettant de tester uniquement si les coefficients sont constants ou non sans hypothèse sur la forme de la rupture (brutale ou non) et sur la date de la rupture.

La modélisation générale de la régression linéaire s'écrit :
\begin{align*}
y_t&=\alpha_{0}x_{0,t}+\alpha_{1} x_{1,t}+\dots+\alpha_{p} x_{p,t} +\varepsilon_t  \\
&= {\bf X_t} \bf\alpha  +\varepsilon_t\\
\E{\varepsilon_t|x_t}&=0 \text{ (exogénéité stricte)} \\
\E{\varepsilon_t^2}&=\sigma_t^2\text{ et } \underset{n\to\infty}{\lim}\frac{1}{n}\sum_{t=1}^n\sigma_t^2=\sigma.
\end{align*}
On suppose également que toutes les variables sont faiblement dépendantes (cas général de la régression linéaire).
<!-- tu as oublié de définir faiblement dépendentes -->
<!-- faiblement dépendantes? tu veux dire faiblement stationnaires? sinon je ne comprends pas ce que ça veut dire -->
<!-- Rep AQLT : Lorsque les variables sont faiblement dépendantes on suppose juste que la corrélation tend vers 0, c'est complémentaire à la stationnarité faible (https://economics.stackexchange.com/questions/42848/stationarity-vs-weak-dependence). 
Par exemple, un AR(1) est faiblement dépendant si le coefficient est strictement inférieur à 1.
Si les variables sont faiblement dépendantes (et pas forcément iid) on peut appliquer le TCL.
C'est moins courant de voir cette notion mais c'est l'hypothèse indiquée dans le papier. -->

Les variables ne doivent donc pas contenir de tendance déterministe ou stochastique (comme des racines unitaires).

Le test consiste à vérifier si l'ensemble des paramètres $(\bf \alpha,\sigma^2)$ sont constants.
L'hypothèse alternative est qu'au moins un paramètre suit une martingale.
<!-- c’est vraiment ça l’hypothèse alternative? que l’espérance d’un coef_s | l’info à t soit égale à ce coef_t ? -->
<!-- Rep AQLT : c'est en tout cas ce qui est mentionné dans leur article :-->
<!-- The test is approximately the Lagrange multiplier test (or locally most powerful test) of the null of constant parameters against the alternative that the parameters follow a martingale.  -->
<!-- This alternative incorporates simple structural breaks of unknown timing as well as random walk parameters. -->

Notons ${\hat \varepsilon}_t =y_t- {\bf X_t} \hat{\bf\alpha}$ et
$$
f_{i,t} = \begin{cases}
x_{i,t}\hat \varepsilon_t &\text{ si }i\leq p\\
\hat \varepsilon_t^2 - \hat \sigma^2&\text{ si }i=p+1
\end{cases}
\text{ et }S_{i,t} = \sum_{j=1}^tf_{i,j}.
$$ 
D'après les conditions de premier ordre $S_{i,n}=0.$

Le test individuel de constance du coefficient du paramètre $i$ est :
$$
L_i=\frac{1}{nV_i}\sum_{t=1}^nS_{i,t}^2\qquad
\text{avec }V_i=\sum_{t=1}^nf_{i,t}^2.
$$ 

Notons :
$$
\bf f_t= \begin{pmatrix}
f_{1,t} \\ \vdots \\ f_{p+1,t}
\end{pmatrix} \text{ et }
\bf S_t= \begin{pmatrix}
S_{1,t} \\ \vdots \\ S_{p+1,t}
\end{pmatrix}.
$$
Le test joint de constance de l'ensemble des paramètres est :
$$
L_c = \frac{1}{n}
\sum_{t=1}^n\transp{\bf S_t}\bf V^{-1}\bf S_t
\text{ avec }\bf V=\sum_{t=1}^n\bf f_{t}\transp{\bf f_{t}}.
$$
Il s'adapte facilement à un test joint de constance d'un sous-ensemble de paramètres en utilisant des sous-vecteurs de $\bf f_t$ et $\bf S_t.$
Toutefois, si le modèle contient des indicatrices alors le test joint ne pourra généralement pas être calculé^[
Dans ce cas, la matrice $\bf V$ n'est pas inversible car la colonne associée à l'indicatrice sera proche de 0.
Si la $i$^e^ variable est une indicatrice à la date $t_0$, $f_{i,t}=x_{i,t}\hat\varepsilon_t$ sera égal à 0 pour $t$ différent de $t_0$ (car $x_{i,t}=0$) et $f_{i,t_{0}}=x_{i,t_{0}}\hat\varepsilon_t=\hat\varepsilon_t\simeq 0$ car l'ajout d'une indicatrice conduit généralement les résidus à être nuls sur la date associée.
].
<!-- 1)	pourquoi « généralement à être nuls » : c’est pas automatique que \hat{epsilon}_{t_0} est rigoureusement nul ? -->
<!-- 2)	dans la formule, tu peux mettre t_0 au lieu de t pour les 2 epsilon, non ? -->

<!-- dessous tu le calcules alors qu’il y a une constante. on ne peut pas avoir 2 indicatrices c’est ça alors? -->
<!-- Rep AQLT : la matrice peut être inversible dans le cas d'une constante, lorsqu'il y a une indicatrice -->
Sous l'hypothèse nulle de constance des paramètres, les $S_{i,t}$ devraient tendre vers 0 (à la manière d'une *tied-down random walk*, c'est-à-dire une marche aléatoire où l'on a contraint la première observation à être égale à la dernière observation) : les statistiques de test $L_i$ et $L_c$ devraient donc être petites.
<!-- c’est quoi une marche aléatoire contrainte? -->
<!-- Rep AQLT : en anglais c'est tied-down random walk, j'ai trouvé une définition ici https://www.researchgate.net/figure/A-tied-down-random-walk-or-Bernoulli-bridge-is-a-simple-random-walk-starting-and-ending_fig1_309729937. C'est une marche aléatoire où l'on a contraint le début et la fin à être égaux. Cela ne me parait pas courant comme notion mais comme c'était mentionné dans l'article je trouvais que ça pouvait être intéressant. Je peux soit supprimer la parenthèse si c'est trop complexe soit expliciter davantage (j'ai fait une proposition dans les parenthèses) -->
Sous l'hypothèse alternative d'instabilité des paramètres, la somme cumulée des $S_{i,t}$ devrait ne pas être de moyenne nulle dans un sous-ensemble de l'échantillon et la statistique de test devrait être élevée.
L'hypothèse nulle de stabilité des coefficients est donc rejetée lorsque la statistique de test est grande.
Sous l'hypothèse nulle, la loi de distribution asymptotique est non standard, les valeurs critiques sont présentées dans la @tbl-hansen-table.

<!-- ça tombe de manière anormalement fréquente sur des 0 en 3e décimale (mais c’est peut-etre normal …) -->
<!-- Rep AQLT : en fait dans la table donné dans l'article c'est 3 chiffres après la virgule pour les valeurs <1 et 2 pour les autres, j'ai donc changé les valeurs en sortie pour éviter la confusion -->
::: {#tbl-hansen-table}
```{r}
#| echo: false
table <- tvCoef::hansen_table
colnames(table) <- gsub("%"," %", colnames(table))
colnames(table) <- gsub(".",",", colnames(table), fixed = TRUE)
colnames(table)[1] <- "Degrés de liberté"
table[,-1] <- apply(table[,-1], 2, function(col){
sapply(col, function(x) {
if (x < 1) {
formatC(x, digits = 3, decimal.mark = ",", format = "f")
} else {
formatC(x, digits = 2, decimal.mark = ",", format = "f")
}
})
})
knitr::kable(table)
```

Source : @hansen1990lagrange.
Table également disponible avec la commande `tvCoef::hansen_table`. 

Valeurs critiques asymptotiques pour $L_c$ en fonction du nombre de paramètres testés (1 degré de liberté pour $L_i$).
:::

Ce test est implémenté dans la fonction `tvCoef::hansen_test()`.
Par défaut, le test joint ne comprend pas le test de constance de la variance (`sigma = FALSE`).

```{r}
hansen_test(reg_lin)
```
Sur notre modèle de prévision de la croissance, le test joint conclut à la non constance des coefficients.
Le test individuel sur le coefficient associé au climat des affaires en différence conclut à sa constance (au seuil de 5 %).
En revanche, le test de Hansen individuel conclut à la non-constance des coefficients associés à la constante et au climat des affaires en niveau au seuil de 5 %.
La non constance de ces deux paramètres peut être vérifiée en utilisant un test joint sur ces deux variables :
```{r}
hansen_test(reg_lin, var = c(1, 2))
```

Le test de Hansen peut être vu comme une extension des tests de stabilité CUSUM (*cumulative sum control chart*) et CUSUM sur les carrés (pour le test sur la variance).
Il est robuste à l'hétéroscédasticité.
En appliquant les mêmes formules au modèle « transformé », ce test est également robuste à la prise en compte de l'autocorrélation via les moindres carrés généralisés.
En revanche, il suppose que toutes les variables sont stationnaires : il ne peut donc directement s'appliquer sur des modèles du type modèle à correction d'erreur.
Dans ce cas, une loi asymptotique différente doit être utilisée^[
Voir par exemple @hansen1992I1.
Une implémentation sous {{< fa brands r-project >}} de ce cas est disponible sous <https://users.ssc.wisc.edu/~bhansen/progs/jbes_92.html>.
].
Si le modèle est estimé en deux étapes par la méthode de @engle1987co, le test peut en revanche s'appliquer sur la seconde estimation (estimation des paramètres de court-terme et de la force de rappel).
<!-- et force de rappel? -->
<!-- Rep AQLT : oui car c'est dans la seconde estimation, je précise dans les parenthèses -->

# Description des méthodes {#sec-desc-meth} 

Si un des tests précédents conclut à la non constance des coefficients du modèle estimé c'est qu'il est mal spécifié et donc qu'il faut utiliser une modélisation alternative qui pourrait notamment provenir d'un problème de variables omises.
Dans cette étude, nous supposons que le problème de spécification provient des observations récentes et qu'il n'est pas nécessaire de faire un ajout de nouvelles variables explicatives pour le régler.
Dans certains cas, par exemple pour prendre en compte la crise du COVID-19, il peut être utile d'ajouter des variables supplémentaires (e.g., des indicatrices).
<!-- utile ou nécessaire? -->
<!-- Rep AQLT : je dirais plutôt utile car d'autres méthodes peuvent être utilisées (utilisation d'une variable externe par exemple) et il peut y avoir certaines séries non affectées par le COVID (du fait de la méthode de mesure par exemple) -->

Trois méthodes sont étudiées dans cette étude :

- la régression linéaire par morceaux (@sec-reg-morceaux) ;

- la régression locale (@sec-reg-locale) ;

- la régression avec coefficients stochastiques (estimés par une modélisation espace-état\ ; @sec-ssm).

## Régression par morceaux {#sec-reg-morceaux}

La régression par morceaux est la modélisation la plus simple : elle consiste à estimer le modèle sur un sous-ensemble des données.
La modélisation est similaire à celle de la procédure de Bai et Perron puisque cette dernière donne directement les « morceaux » : entre les dates de ruptures.

Par exemple, pour le modèle de prévision de la croissance, deux régressions seraient estimés en utilisant les données avant et après  `{r} format(zoo::as.yearqtr(strucchange::breakdates(bp)), "%YT%q")`.

Deux méthodes d'estimations sont possibles :

1. Une régression en une étape est faite en dédoublant les régresseurs en fonction de la date de rupture (fonction `tvCoef::piece_reg()`) :
\begin{align*}
y_t &= \alpha_0\1_{t\leq 2000T3} + \alpha_1 climat\_fr_t\1_{t\leq 2000T3} + \alpha_2 \Delta climat\_fr_t\1_{t\leq 2000T3} + \\
&\phantom{=} \alpha_0'\1_{t > 2000T3} + \alpha_1' climat\_fr_t\1_{t > 2000T3} + \alpha_2' \Delta climat\_fr_t\1_{t > 2000T3} + \varepsilon_t
\end{align*}
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| output: asis -->
<!-- date_rupture <- format(zoo::as.yearqtr(strucchange::breakdates(bp)), "%YT%q") -->
<!-- cat( -->
<!--   sprintf(" -->
<!-- \\begin{align*} -->
<!-- y_t &= \\alpha_0\\1_{t\\leq %s} + \\alpha_1 climat\\_fr_t\\1_{t\\leq %s} + \\alpha_2 \\Delta climat\\_fr_t\\1_{t\\leq %s} + \\\\ -->
<!-- &\\phantom{=} \\alpha_0'\\1_{t > %s} + \\alpha_1' climat\\_fr_t\\1_{t > %s} + \\alpha_2' \\Delta climat\\_fr_t\\1_{t > %s} + \\varepsilon_t -->
<!-- \\end{align*} -->
<!--           ",date_rupture,date_rupture,date_rupture,date_rupture,date_rupture,date_rupture -->
<!--   ) -->
<!-- ) -->
<!-- ``` -->

2. En effectuant deux régressions linéaires distinctes (fonction `tvCoef::bp_lm()`) :
$$
\begin{cases}
\forall t \leq 2000T3 :\quad y_t = \alpha_0 + \alpha_1 climat\_fr_t + \alpha_2 \Delta climat\_fr_t + \varepsilon_t \\
\forall t > 2000T3 :\quad y_t = \alpha_0' + \alpha_1' climat\_fr_t + \alpha_2' \Delta climat\_fr_t + \varepsilon_t'
\end{cases}.
$$
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| output: asis -->
<!-- cat( -->
<!--   sprintf("$$ -->
<!-- \\begin{cases} -->
<!-- \\forall t \\leq %s :\\quad y_t = \\alpha_0 + \\alpha_1 climat\\_fr_t + \\alpha_2 \\Delta climat\\_fr_t + \\varepsilon_t \\\\ -->
<!-- \\forall t > %s :\\quad y_t = \\alpha_0' + \\alpha_1' climat\\_fr_t + \\alpha_2' \\Delta climat\\_fr_t + \\varepsilon_t' -->
<!-- \\end{cases}. -->
<!-- $$          ",date_rupture,date_rupture -->
<!--   ) -->
<!-- ) -->
<!-- ``` -->

Dans les deux cas les coefficients estimés sont les mêmes mais les écarts-types seront en général différents.
En effet, dans la première modélisation on suppose que la variance du résidu est constante sur l’ensemble de la période alors que dans la seconde la variance est supposée constante sur les deux sous-périodes.
Par défaut les fonctions `tvCoef::piece_reg()` et `tvCoef::bp_lm()` calculent les dates de ruptures en appliquant la fonction `strucchange::breakdates()` sur le modèle de régression linéaire en paramètre.
Cette date de rupture peut toutefois être manuellement spécifiée en utilisant le paramètre `break_date`.
<!-- précise comment piece_reg() connaît la date de rupture: je n’ai pas vu quelle partie du code l’identifie: est-ce qu’elle doit être dans reg_lin, ou alors piece_reg la calcule lui-même? idem pour bp_lm() -->
<!-- Rep AQLT : j'ai ajouté deux phrases pour le préciser : c'est calculé automatiquement par défaut -->
```{r}
reg_morc <- piece_reg(reg_lin)
bp_lm <- bp_lm(reg_lin)
coef(reg_morc$model)
c(coef(bp_lm$model[[1]]), coef(bp_lm$model[[2]]))
```

Dans la majorité des cas, la variance des erreurs n’a pas de raison d’être différente selon la période considéree, et nous suggérons de privilégier la première modélisation car elle offre plus de flexibilité, notamment pour fixer les coefficients de certaines variables.

Dans notre exemple, le test d'Hansen concluait à la constance du coefficient du climat des affaires en différence : les coefficients estimés avant et après la rupture devraient donc être égaux.
Cette égalité peut être testée en utilisant un test de Fisher, par exemple avec la fonction `car::linearHypothesis()` [@car].
Ici on rejette l'hypothèse nulle d'égalité de tous les coefficients avant et après la rupture : la prise en compte de la rupture est donc justifiée.
On ne rejette pas l'hypothèse nulle d'égalité du coefficient du climat des affaires en niveau seulement, le modèle pourrait donc être simplifié.
<!-- Même lorsque le modèle peut être simplifié, nous conseillons de toujours considérer que la constante varie dans le temps, même lorsque la différence des coefficients associés à la constante n'est pas statistiquement significative\ : cela permet de s'assurer que le modèle ne sera pas biaisé par des résidus qui ne seraient pas de moyenne nulle. -->
<!-- je ne suis pas sûr de comprendre pourquoi: avec une constante dans la régression, les résidus sont nuls en moyenne par construction, non? -->
<!-- Rep AQLT : Effectivement, du coup je ne vois pas de raison de scinder la constante et on peut enlever cette phrase ? -->

```{r}
# On rejette l'hypothèse nulle de constance de tous les coefficients
car::linearHypothesis(
  reg_morc$model,
  c(
    "`(Intercept)_2000.5` = `(Intercept)_2019.75`",
    "bc_fr_m1_2000.5 = bc_fr_m1_2019.75",
    "diff_bc_fr_m1_2000.5 = diff_bc_fr_m1_2019.75"
    )
  ,
  test = "F")
# On ne rejette pas (H0) constance du climat des affaires en différence
car::linearHypothesis(
  reg_morc$model,
  c(
    "diff_bc_fr_m1_2000.5 = diff_bc_fr_m1_2019.75"
    ),
  test = "F")
# Pour les autres variables, faire attention à l'interprétation des tests 
# individuels : on ne rejette pas (H0) constance du climat des affaires
# en niveau
car::linearHypothesis(
  reg_morc$model,
  c(
    "bc_fr_m1_2000.5 = bc_fr_m1_2019.75"
    ),
  test = "F")
# On ne rejette pas (H0) constance de la constante
car::linearHypothesis(
  reg_morc$model,
  c(
    "`(Intercept)_2000.5` = `(Intercept)_2019.75`"
    ),
  test = "F")
# On rejette (H0) constance de la constante + climat des affaires en niveau
car::linearHypothesis(
  reg_morc$model,
  c(
    "`(Intercept)_2000.5` = `(Intercept)_2019.75`",
    "bc_fr_m1_2000.5 = bc_fr_m1_2019.75"
  ),
  test = "F")
# Pour fixer le coefficient associé au climat des affaires en niveau
reg_morc2 <- piece_reg(reg_lin, fixed_var = 2)
coef(reg_morc2$model)
```

La qualité prédictive du nouveau modèle peut s'apprécier de plusieurs façons, la plus classique étant la minimisation du critère d'information d'Akaike (AIC et fonction `AIC()`) ou la minimisation des erreurs de prévision hors échantillon (également appelées pseudo temps-réel, fonction `tvCoef::oos_prev()`).
Pour le calcul des erreurs de prévision hors échantillon, la méthodologie retenue consiste à calculer pour chaque date $t$ la prévision obtenue à la date $t+1$ en estimant le modèle à partir des observations disponibles jusqu'à la date $t$ uniquement.
Avec cette méthode, appelée le *leave-one-out cross-validation*, on ne s'intéresse donc qu'à la qualité de prévision à l'horizon d'un trimestre (ce qui est le cas d'utilisation pour les modèles étudiés).
Par ailleurs, minimiser l'AIC est asymptotiquement équivalent à minimiser ces erreurs de prévision hors échantillon [@AIC].

Sur notre exemple, et par rapport à la régression supposant des coefficients constants dans le temps, la régression linéaire par morceaux permet de minimiser ces deux critères\ :

```{r}
# AIC minimisé :
AIC(reg_morc$model) < AIC(reg_lin)
oos_reg_morc <- oos_prev(reg_morc)
oos_lm <- oos_prev(reg_lin)
res_oos <- ts.union(oos_lm$residuals, oos_reg_morc$residuals)
# Les deux modèles étant équivalents avant la rupture,
# on n'étudie les prévisions qu'après celle-ci
res_oos <- window(res_oos, start = 2003)  
# erreurs de prévision hors échantillon minimisées
apply(res_oos, 2, rmse)
```
<!-- totalement optionnel: ajouter une barre verticale dans le graphique par morceaux pour la date à laquelle la 2e période commence. -->
<!-- Rep AQLT : C'est fait -->

La @fig-prev-piecereg montre les erreurs de prévision hors échantillon des deux modèles étudiés.
Autour de la date de rupture, la régression linéaire par morceaux produit des prévisions peu réalistes (ce qui conduit à des erreurs élevées) : cela s'explique par le fait que très peu d'observations sont utilisées pour estimer les coefficients associés aux régresseurs après la rupture, les estimateurs sont donc peu précis (grande variance).
Par ailleurs, lors d'un vrai exercice en temps réel, la date de rupture ne sera généralement connue et prise en compte que plusieurs trimestres après celle-ci : l'erreur est alors réduite.
Pour les analyses automatiques hors échantillon, il faut donc faire attention aux valeurs prédites autour de la rupture !
<!-- ça rejoint aussi un autre aspect de la question, c’est qu’il faut un certain temps avant d’être sûr qu’il y a eu une rupture, parce que s’il y a des erreurs importantes quelques trimestres, ça peut être des erreurs temporaires, ou alors un changement structurel, mais dur à dire en temps réel! -->
<!-- Rep AQLT : Oui effectivement, j'ai rajouté une phrase pour donner cet argument supplémentaire -->


```{r}
#| echo: false
#| fig-cap: Erreurs de prévision de la croissance du PIB à partir d'un modèle de régression linéaire et d'un modèle de régression linéaire par morceaux.
#| label: fig-prev-piecereg
# prev <- ts.intersect(gdp[,"growth_gdp"], oos_lm$forecast, oos_reg_morc$forecast)
res_oos <- ts.union(oos_lm$residuals, oos_reg_morc$residuals)
range_res <- range(window(res_oos, start = 2000))
data <- data.frame(as.numeric(time(res_oos)),
                   res_oos)
colnames(data) <- c("time", "Rég. lin.",
                    "Rég. par morceaux")
p1 <- ggplot(data = data, aes(x = time, y = `Rég. lin.`)) +
  geom_bar(stat = "identity",
           fill = colors_graph[1]) +
  labs(x = NULL, y = NULL, title = colnames(data)[2])
p2 <- ggplot(data = data, aes(x = time, y = `Rég. par morceaux`)) +
  geom_vline(xintercept = strucchange::breakdates(bp),linetype = "dotted") + 
  annotate("text", 
           label = format(zoo::as.yearqtr(strucchange::breakdates(bp)), "%YT%q"),
           x = strucchange::breakdates(bp), y = -1,
           hjust = 1, vjust = 1, 
           size = 2.5, 
           color = "black") +
  geom_bar(stat = "identity",
           fill = colors_graph[2]) +
  labs(x = NULL, y = NULL, title = colnames(data)[3])
(p1 / p2) &
  scale_x_continuous(breaks = seq.int(2000, 2020, 2)) & 
  coord_cartesian(ylim = range_res, xlim = c(2000, 2020)) &
  theme_bw()
```

Comme indiqué dans la @sec-test-baiperron, l'inconvénient de cette méthode provient du choix de la date de rupture lorsque celle-ci n'est pas imposée par l'utilisateur.
La @fig-temps-reel-bp montre la date de la rupture détectée par la procédure de Bai et Perron en fonction de la date de fin d'estimation du modèle de régression linéaire : aucune rupture n'est détectée avant 2009 ou lorsque le modèle est estimé en utilisant des données jusqu'en 2012T3-2016T1.
En fonction de la date de fin d'estimation, la rupture détectée automatiquement peut tout aussi bien être en 2000 qu'en 2001, 2004 ou 2006.
Même s'il est possible que cela n'ait que très peu d'effet sur les prévisions estimées en fin de période, l'interprétation faite du modèle sera vraisemblablement différente !


```{r}
#| echo: false
#| label: fig-temps-reel-bp
#| fig-cap: Date de rupture détectée par l'algorithme de Bai et Perron en fonction de la date de fin d'estimation du modèle.
# On enlève les premières observations car pas de rupture
bp_hist <- lapply(oos_lm$model[-(1:74)], breakpoints)
bpd <- sapply(bp_hist, breakdates)
bpd <- bpd * sapply(bp_hist, `[[`, "nobs")
bp_ts <- ts(time(get_data(reg_morc))[bpd], 
            end = end(oos_lm$forecast),
            frequency = frequency(oos_lm$forecast))
ggplot(data = na.omit(data.frame(x = as.numeric(time(bp_ts)),
                                 y = as.numeric(bp_ts))),
       aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  coord_cartesian(xlim =c(2009,NA)) +
  scale_x_continuous(breaks = seq(2010,2020, by = 2))+
  labs(x = "Date de fin d'estimation",
       y = "Date de rupture détectée")
```



## De la régression mobile à la régression locale {#sec-reg-locale}

La régression mobile est une des méthodes empiriques les plus simples pour savoir si les coefficients évoluent dans le temps.
Celle-ci consiste à estimer des régressions sur des fenêtres glissantes et à observer la courbe des coefficients estimés.
En reprenant notre exemple où les données commencent en 1980, avec une fenêtre fixe de 10 ans (par exemple), cela consiste à estimer une première régression entre 1980T1 et 1989T4, une deuxième entre 1980T2 et 1990T1... et une dernière entre 2010T1 et 2019T4.
Sous R cela peut par exemple s'estimer en utilisant la fonction `roll::roll_lm()` [@roll] :
```{r}
roll_lm <- roll::roll_lm(
  x = data_gdp[, c("bc_fr_m1", "diff_bc_fr_m1")],
  y = data_gdp[, "growth_gdp"],
  width = 4 * 10
)
coef_roll_lm <- ts(roll_lm$coefficients, start = 1980, frequency = 4)
```

La @fig-coef-rollreg montre les coefficients estimés par cette régression mobile.
Seuls ceux estimés sur le climat des affaires en différence montrent une rupture nette.
Elle s'observe à partir de 2005, lorsque plus de la moitié des points de la fenêtre (5 ans) sont estimés après la date de rupture détectée (2000T3).
<!-- et ce qui doit jouer aussi, c’est les points 2009 qui doivent être très extrêmes et tirer les coefficients... -->
<!-- Rep AQLT : Effectivement, rupture à partir de 2009 mais on a l'impression de voir -->

::: {#fig-coef-rollreg}
```{r}
#| echo: false
#| label: ffig-coef-rollreg
coef_roll_lm <- na.omit(coef_roll_lm)
coef_morc <- coef(reg_morc)
coef_lin <- ts(matrix(coef(reg_lin), nrow = 1), 
               start = start(coef_morc),
               end = end(coef_morc),
               frequency = frequency(coef_morc))
colnames(coef_lin) <- names(coef(reg_lin))

coef2plot(
  list(
    "Rég. linéaire" = coef_lin,
    "Rég. par morceaux" = coef_morc,
    "Rég. mobile" = coef_roll_lm
  ),
  start = start(coef_roll_lm)
)
```

Lecture : la régression mobile est estimée sur une fenêtre de 10 ans.
Les coefficients estimés en 2000T1 correspondent aux coefficients estimés entre 1980T2 et 2000T1.

Coefficients estimés par régression linéaire, régression par morceaux et régression mobile.
:::

La régression mobile a l'avantage d'être très simple mais repose sur plusieurs paramètres qui ont ici été fixés arbitrairement dont notamment :

- La longueur de la fenêtre : elle doit être suffisamment large pour avoir des bonnes estimations mais suffisamment courte afin de permettre de prendre en compte les ruptures.

- La date à laquelle les coefficients sont associés.
Dans la fonction `roll::roll_lm()` ils sont associés à la dernière date de la fenêtre : les coefficients de la date $t$ correspondent à ceux obtenus en utilisant les données jusqu'à la date $t.$
Ils auraient également pu être associés à la première date de la fenêtre ou encore à son milieu (coefficients de la date $t$ estimés en utilisant autant d'observations avant et après $t$).
Dans tous les cas une stratégie doit être adoptée afin de gérer les observations manquantes (dans notre exemple il s'agit donc d'estimer les coefficients avant 1989).

La régression locale permet, grâce à une modélisation plus poussée, de donner des solutions à ce problème.
Dans ce papier nous détaillons la modélisation utilisée dans la fonction `tvReg::tvLM()` développée par @tvReg^[
D'autres packages sont disponibles pour effectuer une régression locale, dont par exemple `locfit` de @locfit.
Toutefois, nous avons ici privilégié le package `tvReg` du fait de sa simplicité d'utilisation et parce qu'il implémente également une fonction `tvReg::tvAR()` qui permet de prendre en compte de manière optimale les retards de la variable endogène (cas non étudié dans cette étude).
].
On suppose ici que les coefficients $\bf\alpha_t$ de l'@eq-mod-fixe dépendent d'une variable aléatoire $z_t$ : $\bf\alpha_t=\alpha(z_t).$
<!-- peut-etre faire référence à la formule avec alpha_t qui est au début de la partie 2. c’est le seul endroit (sauf erreur) où tu as cette notation avec alpha_t, et le lecteur peut l’avoir oublié ou ne pas avoir lu cette partie, il faut qu’il soit au clair sur ce dont tu veux parler. -->
<!-- Rep AQLT : une référence est ajoutée -->
Par défaut $z_t=t/T$ avec $T$ le nombre d'observations : les coefficients dépendent donc d'une mesure normalisée du temps.
On suppose que la fonction $\alpha$ est localement constante ($\alpha(z_t)\simeq \alpha(z)$, option par défaut) ou localement linéaire  ($\alpha(z_t)\simeq \alpha(z)+\alpha'(z)(z_t-z)$), c'est-à-dire que pour toute date $t$ on a pour toute date $i$ proche de $t$ : $\alpha(z_i)\simeq\alpha(z_t)$ ou $\alpha(z_i)\simeq\alpha(z_t)+\alpha'(z_t)(z_i-z_t).$ 
Cette approximation locale est justifiée par le théorème de Taylor.

Pour chaque date $t$, le coefficient $\alpha_t=\alpha(z_t)$ est obtenu par moindres carrés pondérés.
Lorsque $\alpha$ est supposé localement constant il s'agit du système :
$$
\hat{\bf\alpha_t}=\hat{\alpha}(z_t)=\underset{\bf\theta_0}\argmin\sum_{i=1}^T\left[y_i-{\bf X_i}\bf\theta_0 \right]^2K_{b_t}(z_i-z_t).
$$
Lorsque $\alpha$ est supposé localement linéaire il s'agit du système :
$$
(\hat{\alpha}(z_t), \hat{\alpha}'(z_t))=\underset{\bf \theta_0,\bf \theta_1}\argmin\sum_{i=1}^T\left[y_i-{\bf X_i}\bf\theta_0 - (z_i-z_t){\bf X_i}\bf\theta_1\right]^2K_{b_t}(z_i-z_t).
$$
Avec $K_{b_t}(z_i-z_t)=\frac{1}{b_t}K\left(\frac{z_i-z_t}{b_t}\right)$ et $K(\cdot)$ une fonction de noyau.
La fonction $K$ permet de pondérer les observations : pour l'estimation du coefficient à la date $t$ on accorde généralement plus d'importance (i.e., un poids plus important) aux observations qui sont proches de $t$ qu'à celles qui sont éloignées de $t.$
C'est une fonction positive, paire et intégrable telle que $\int_{-\infty}^{+\infty}K(u) \ud u=1.$
Trois noyaux sont disponibles dans la fonction `tvReg::tvLM()` :

- Le cubique (*triweight*, utilisé par défaut) :
$$
K(u)=\frac{35}{32}\left(
1-
\left\lvert
u
\right\lvert^2
\right)^3\1_{[-1,1]}(u).
$$

- Le noyau d'Epanechnikov (ou parabolique) :
$$
K(u)=\frac{3}{4}\left(
1-
\left\lvert
u
\right\lvert^2
\right)\1_{[-1,1]}(u).
$$

- Le noyau Gaussien :
$$
K(u)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}u^2\right).
$$

Le paramètre $b_t$ permet de calibrer la largeur de la fenêtre (i.e., le nombre de points utilisés pour chaque estimation).
Il est généralement supposé constant ($b_t=b$).

Dans notre exemple de prévision du PIB, $T=160$ observations sont utilisées.
Avec $z_t=t/T$ et indexant chaque observation entre 1 et $T,$ la régression mobile sur 15 ans où l'on affecte le coefficient de la date $t$ au milieu de la fenêtre d'estimation est donc retrouvée en utilisant le noyau uniforme $K(u)=\1_{[-1,1]}(x)$ avec $b_t=b=\frac{30}{160}.$
En effet, dans ce cas $K(z_t-z_i)\ne0$ si et seulement si $|t-i|\leq30$ : on utilise donc 30 observations (soit $7,5$ ans) de chaque côté de $t$ pour estimer le coefficient à la date $t.$

Dans `tvReg`, le paramètre $b$ est par défaut obtenu en minimisant une statistique de validation croisée dans l'intervalle $\left[\frac{5}{T},20\right].$
Lorsque la valeur par défaut de $b$ est plus grande que 1, toutes les observations sont utilisées pour l'estimation de chaque coefficient $\bf\alpha_t.$
Plus $b$ se rapproche de 1 plus on se rapproche du cas de la régression linéaire puisque dans ce cas les poids donnés par $K$ tendent à être constants pour toutes les observations.
En effet, dans ce cas, pour $T=160,$ $\frac{\max_u K(u)}{\min_u K(u)}$ est compris entre $1,001$ et $1,008$ pour les noyaux cubiques, paraboliques et gaussiens.

Reprenons notre exemple de prévision du PIB avec une détection automatique de la fenêtre.

```{r}
reg_loc <- tvReg::tvLM(
  formula = growth_gdp ~ bc_fr_m1 + diff_bc_fr_m1,
  data = data_gdp
)
summary(reg_loc)
```
La fenêtre estimée par défaut est de `{r} formatC(reg_loc$bw, digits = 2, decimal.mark = ",")`, c'est-à-dire que pour estimer le coefficient à la date $t$ on utilise au plus `{r} round(reg_loc$bw*nrow(reg_lin$model)/4)` ans avant et après $t$ : on utilise tous les points dans la majorité des cas.
Cela explique le caractère très lisse des coefficients (@fig-coef-reg-mobile). 
Bien qu’à n’importe quelle date entre 1990 et 2010, tous les points sont utilisés pour estimer le coefficient correspondant, les poids associés à ces points varient d’une date à l’autre, si bien que le coefficient estimé n’est pas constant sur cette période.
Avec ce paramètre pour la fenêtre, les ruptures brutales sont donc difficiles à prendre en compte.


```{r}
#| echo: false
#| label: fig-coef-reg-mobile
#| fig-cap: Coefficients estimés par régression linéaire, régression par morceaux et régression locale (avec $b=0,75$).
# coef_roll_lm <- ts(coef_roll_lm, start = 1980+15/2-0.25,
#                        frequency = 4)
coef_reg_loc <- ts(coef(reg_loc), start = 1980, frequency = 4)

coef2plot(
  list(
    "Rég. linéaire" = coef_lin,
    # "Rég. mobile" = coef_roll_lm,
    "Rég. par morceaux" = coef_morc,
    "Rég. locale" = coef_reg_loc
  )
)
```
<!-- ref à Loader utilise son prénom (modifier le .bib?) -->
<!-- Rep AQLT : En fait c'est quelque chose d'automatique pour différencier la référence à clive loader (livre sur la régression locale) et catherine loader (package R, notamment mentionné dans le premier livre). -->
<!-- Je n'avais jamais remarqué la différence car généralement on ne voit que l'initiale du prénom. -->
<!-- Je me demande si ce n'est pas une erreur dans le package parce que l'auteur initial du package est Clive Loader et lorsque le mainteneur a changé le prénom a été changé en Catherine Loader : j'ai envoyer un mail au mainteneur pour poser la question. -->
Un des inconvénients de la méthode de sélection automatique de la fenêtre est que le critère utilisé (statistique de validation croisée) est peu discriminant [voir notamment @Loader1999] : ce critère peut prendre des valeurs très proches pour différentes valeurs de la fenêtre alors que celle-ci a un impact fort sur l'interprétation du modèle !
Cela a également pour effet que la méthode est peu stable dans le temps (@fig-oos-bw), ce qui augmente les sources de révisions des simulations hors échantillon, calculables en utilisant la fonction `oos_prev()` :

```{r}
oos_reg_loc <- oos_prev(reg_loc)
oos_bw <- ts(sapply(oos_reg_loc$model, `[[`,"bw"),
             end = c(2019, 4),
             frequency = 4)
```


```{r}
#| echo: false
#| label: fig-oos-bw
#| fig-cap: "Fenêtre $b$ détectée automatiquement en fonction de la date de fin d'estimation du modèle."
suppressMessages(
  autoplot(oos_bw) +
  # geom_abline(slope = 0, intercept = 1) +
  theme_bw() +
  labs(x = "Date de fin d'estimation",
       y = latex2exp::TeX("Fenêtre $b$")) +
  scale_x_continuous(breaks = seq.int(1990, 2020, by = 5))
)
```

L'estimation en temps réel revient à utiliser une fonction de noyau tronquée : plus de points dans le passé que dans le futur sont utilisés pour estimer les derniers coefficients.
C'est donc également une source de révision au fur et à mesure que des nouveaux points seront connus.
Même si des méthodes optimales existent pour minimiser les erreurs d'estimation des coefficients en temps réel [voir par exemple @FengSchafer2021], cela devrait ici avoir peu d'impact car un modèle très simple est ici utilisé pour estimer les coefficients (approximation de la fonction $\alpha$ par une constante).

Un autre inconvénient de ces méthodes est que tous les coefficients varient dans la temps alors que dans certains cas on peut supposer la relation constante.
<!-- Pour fixer certains coefficients, on peut procéder comme dans la @sec-test-baiperron en faisant une régression linéaire pour estimer les coefficients fixes, suivie d'une régression mobile. -->
<!-- vérifier si la formulation est ok avec les changements qui ont été apportés suite à mon commentaire -->
<!-- Rep AQLT : Supprimé comme vu avec toi -->

## Régression avec coefficients stochastiques (modélisation espace-état) {#sec-ssm}

La modélisation espace-état est une méthodologie générale permettant de traiter un grand nombre de problèmes de séries temporelles.
Dans cette approche, on suppose que tout modèle est déterminé par une série de vecteurs non observés $\bf \alpha_1,\dots,\bf\alpha_T$ associés aux observations $y_1,\dots,y_T$, la relation entre $\alpha_t$ et $y_t$ étant spécifiée par le modèle espace-état.
Ces modèles sont largement décrits dans la littérature, notamment par @durbinkoopman.
Dans cette étude, nous nous placerons dans un cadre simplifié des modèles linéaires gaussiens appliqués aux régressions linéaires.
Les modèles sont déterminés par un ensemble de deux équations :
$$
\begin{cases}
y_t={\bf X_t}\bf\alpha_t+\varepsilon_t,\quad&\varepsilon_t\sim\mathcal N(0,\sigma^2)\\
\bf\alpha_{t+1}=\bf\alpha_t+\bf\eta_t,\quad&\bf\eta_t\sim\mathcal N(\bf 0,\bf\Sigma)
\end{cases},\text{ avec }\eta_t\text{ et }\varepsilon_t\text{ indépendants.}
$$
La première équation est l'équation d'observation (*observation equation*), la seconde l'équation d'état (*state equation*) et $\bf\alpha_t$ le vecteur d'états (*state vector*).

Dans cette étude, la matrice de variance-covariance $\bf\Sigma$ est supposée diagonale : la dynamique d'évolution des coefficients d'une variable est donc indépendante de la dynamique d'évolution des autres variables.
Lorsque des contraintes entre les différents coefficients existent, des spécifications différentes de la matrice de variance-covariance $\bf\Sigma$ peuvent être faites : c'est par exemple ce qui a été fait par @abs2006 pour estimer des coefficients jours ouvrables variant dans le temps.
Chaque coefficient suivant une marche aléatoire, nous appelons cette méthode *modèle de régression avec coefficients stochastiques*.

On retrouve le cas de la régression linéaire lorsque $\bf\Sigma=\bf 0$ puisque dans ce cas tous les $\alpha_t$ sont égaux.

Ces modèles sont implémentés dans la fonction `tvCoef::ssm_lm()` qui prend en entrée un modèle de régression linéaire.
Elle s'appuie sur le package `rjd3sts` [@rjd3sts] qui permet d'implémenter très facilement les modèles espace-état sans devoir écrire explicitement le modèle.
Par défaut les variances du vecteur d'états ($\bf \Sigma$) ne sont pas estimées et sont fixées à 0 : on retrouve donc les coefficients estimés par régression linéaire.
<!-- quel pluriel à espace-état? on ne le garderait pas au singulier? -->
<!-- Rep AQLT : J'ai l'impresison que ça se garde au singulier en effet même si on peut écrire espace-états (car un espace pour plusieurs états). -->

```{r}
ssm <- ssm_lm(reg_lin)
summary(ssm)
```

L'estimation des hyperparamètres (variances des bruits blancs $\eta_t$ et $\varepsilon_t$) est faite par maximum de vraisemblance, et différentes méthodes existent pour initialiser les modèles (calculer $\bf \alpha_1$).
Pour plus de détails voir par exemple @durbinkoopman.
Le filtre de Kalman permet ensuite de calculer tous les coefficients.
Parmi les paramètres calculés, les deux principaux sont : 

1. Les états lissés (*smoothed states*) $\E{\alpha_t|y_1,\dots,y_n}$ : il s'agit de l'estimation des états ($\bf\alpha_t$) en utilisant toute l'information disponible. 
Dans le cadre de la régression linéaire, les états lissés sont donc constants sur toutes les dates et correspondent aux coefficients estimés en utilisant l'ensemble des données disponibles :

```{r}
window(ssm$smoothed_states, start = 2019)
```

2. Les états filtrés (*filtered states*) $\E{\alpha_t|y_1,\dots,y_{t-1}}$ : il s'agit de l'estimation des états ($\bf\alpha_t$) en utilisant l'information disponible jusqu'à la date précédente.
Dans le cadre de la régression linéaire, cela correspond aux coefficients estimés hors échantillon : la valeur des états filtrés en 2010T2 correspond aux coefficients estimés en utilisant les données jusqu'au 2010T1.
Ils permettent donc d'avoir une estimation des prévisions hors échantillon du modèle.
```{r}
round(window(ssm$filtering_states, start = c(2010, 2), end = c(2010, 2)), 6)
round(coef(dynlm(
  formula = growth_gdp ~ bc_fr_m1 + diff_bc_fr_m1,
  data = window(gdp, start = 1980, end = c(2010,1))
)), 6)
```
Lorsque les variances sont estimées, les états filtrés ne correspondent pas exactement à des estimations hors échantillon car les hyperparamètres restent fixés (variances $\bf\Sigma$ et initialisation).
Les estimations hors échantillon peuvent être calculées en utilisant la fonction `tvCoef::ssm_lm_oos()`.

Pour faciliter l'estimation des variances $\bf\Sigma,$ le modèle est souvent reparamétré :
$$
\begin{cases}
y_t={\bf X_t}\bf\alpha_t+\varepsilon_t,\quad&\varepsilon_t\sim\mathcal N(0,\sigma^2)\\
\bf\alpha_{t+1}=\bf\alpha_t+\bf\eta_t,\quad&\bf\eta_t\sim\mathcal N(\bf 0,\sigma^2\bf Q)
\end{cases},\text{ avec }\eta_t\text{ et }\varepsilon_t\text{ indépendants.}
$$
Les variances sont donc définies à un facteur multiplicatif près et une estimation en deux étapes est faite : la vraisemblance est dite *concentrée*.
C'est ce qui est utilisé par défaut dans `tvCoef::ssm_lm()`.
Dans notre exemple, l'erreur standard de la régression (*residual standar error*) peut se calculer de la façon suivante :

```{r}
sqrt(ssm$parameters$parameters * ssm$parameters$scaling)
summary(reg_lin)$sigma
```

Afin d'estimer les variances associées à l'équation d'état, il faut utiliser les paramètres `fixed_var_intercept = FALSE` et `fixed_var_trend = FALSE`.
Même si la valeur des variances (`var_intercept` et `var_variables` qui valent 0 par défaut) n'aura aucun effet sur les variances finales estimées, il est parfois nécessaire de modifier ces valeurs afin d'éviter une erreur dans l'optimisation.
Sur notre modèle, l'optimisation conduit à garder fixe le coefficient associé au climat des affaires en niveau (variance nulle) mais considère que les autres variables varient dans le temps :

```{r}
ssm <- ssm_lm(reg_lin, 
              fixed_var_intercept = FALSE, 
              fixed_var_variables = FALSE,
              var_intercept = 0.01,
              var_variables = 0.01)
sqrt(ssm$parameters$parameters * ssm$parameters$scaling)
```

::: {.remark}
Dans la version actuelle de `tvCoef`, les retards de la variable endogène (à prévoir) ne sont pas modélisés correctement.
En effet, dans ce cas il faudrait utiliser une modélisation différente afin de prendre en compte la relation entre la variable endogène et les retards.
:::


La @fig-coef-ssm montre les coefficients estimés avec toutes les méthodes présentées dans ce papier.
Pour toutes les méthodes, le coefficient du climat des affaires en niveau est stable dans le temps (coefficients estimés compris entre 0,020 et 0,023).
En revanche, les résultats de la régression avec coefficients stochastiques sont sensiblement différents pour le coefficient associé à la variation du climat des affaires, avec des périodes où le coefficient est plus faible que pour les autres méthodes (avant 1983, entre 1995 et 1999 et après 2011) et d'autres où il est plus élevé (notamment pendant la crise financière).

```{r}
#| echo: false
#| label: fig-coef-ssm
#| fig-cap: Coefficients estimés par régression linéaire, régression par morceaux, régression locale (avec $b=0,75$) et régression avec coefficients stochastiques (modélisation espace-état).

coef_ssm <- coef(ssm)
all_coef <- list(
    "Rég. linéaire" = coef_lin,
    "Rég. par morceaux" = coef_morc,
    "Rég. locale" = coef_reg_loc,
    "Coef. stochastiques\n(espace-état)" = coef_ssm
  )
# round(range(sapply(all_coef, function(x) x[,"bc_fr_m1"])),3)

coef2plot(
  all_coef
)
```

La @tbl-res-model-pib compare la qualité prédictive des différents modèles dans l'échantillon (en utilisant toutes les données pour estimer les paramètres des modèles) et hors échantillon (reproduction du processus de prévision en estimant de manière récursive les modèles jusqu'à la date $t$ pour calculer les prévisions à la date $t+1$).
C'est la régression avec coefficients stochastiques (modélisation espace-état) qui minimise les erreurs de prévision, suivie de la régression par morceaux.
La régression locale a une erreur hors échantillon plus élevée notamment du fait des instabilités sur l'estimation de la fenêtre.
Le test de Diebold-Mariano (voir notamment @DMtest), implémenté dans la fonction `forecast::dm.test()` [@forecastR], permet de tester si cette différence est significative.
Dans et hors échantillon, la régression avec coefficients stochastiques a des erreurs de prévision significativement plus petites que la régression linéaire (p-valeurs de 0,00 et 0,05).
Dans l'échantillon elles sont également significativement plus petites que celles de la régression par morceaux (p-valeur de 0,00) mais la différence n'est pas significative hors échantillon (p-valeur de 0,08). Sur les périodes récentes, du fait du coefficient sur le climat des affaires en différence, l'interprétation économique et les prévisions sont différentes.
<!-- tu ne décris pas les prévisions finalement, ni en quoi elles diffèrent, laquelle on aurait envie de croire (celle qui a les erreurs les plus faibles depuis 2012 par ex) -->
<!-- Rep AQLT : En fait ici je suis dans un exercice proche de la sélection de modèles : je cherche donc juste à savoir quel est le meilleur modèle (en termes d'erreurs de prévision) et je ne fais pas d'analyse économique des coefficients estimés. -->
<!-- C'est pour ça que dans la conclusion je suis un peu plus nuancé en disant que le choix du modèle doit être raisonné. -->

```{r}
oos_ssm <- ssm_lm_oos(reg_lin, fixed_var_intercept = FALSE, 
                      fixed_var_variables = FALSE,
                      date = 70)

res_is <- ts.union(
  ts(residuals(reg_lin), end = c(2019,4), frequency = 4),
  residuals(reg_morc),
  ts(residuals(reg_loc), end = c(2019,4), frequency = 4),
  residuals(ssm)[,"smoothed"]
)
res_oos <- ts.union(
  oos_lm$residuals,
  oos_reg_morc$residuals,
  ts(oos_reg_loc$residuals, end = c(2019,4), frequency = 4),
  oos_ssm$oos_noise
)
res_oos <- window(res_oos, start = 2003)
# (H0) : Reg. coef stochastique meilleure que modèle linéaire
forecast::dm.test(res_is[, 1], res_is[, 4], "greater")
# (H0) : Reg. coef stochastique meilleure que régression par morceaux
forecast::dm.test(res_is[, 2], res_is[, 4], "greater")
# (H0) : Reg. coef stochastique meilleure que modèle linéaire
forecast::dm.test(res_oos[, 1], res_oos[, 4], "greater")
forecast::dm.test(res_oos[, 2], res_oos[, 4], "greater")
```



::: {#tbl-res-model-pib}
```{r}
#| echo: false
table <- data.frame(apply(res_is, 2, rmse),
                    apply(res_oos, 2, rmse))
colnames(table) <- c("Dans l'échantillon", "Hors échantillon")
rownames(table) <- c(
  "Régression linéaire",
  "Régression par morceaux",
  "Régression locale",
  "Régression avec coefficients stochastiques (espace-état)")
knitr::kable(table, format.args = list(decimal.mark = ",", digits = 2))
```

Note : les prévisions dans l'échantillon sont calculées en estimant les modèles à partir des données disponibles entre 1980T1 et 2019T4.
Les prévisions hors échantillon sont calculées à partir de 2003T1 : la première prévision (2003T1) correspond à celle que l'on aurait eu en estimant les modèles à partir des données disponibles jusqu'en 2002T4 (trimestre précédente).

Erreurs quadratiques moyennes des erreurs de prévision entre les différentes méthodes.
:::



## Prise en compte de la période du COVID-19 et prévision

Dans les sections précédentes, les modèles ont été estimés jusqu'en 2019T4 dans le but de simplifier la présentation des modèles.
Toutefois, si l'on veut effectuer de la prévision sur les périodes récentes, il est indispensable de prendre en compte la période du COVID-19.
Cela se fait généralement en ajoutant, dans le modèle de prévision, des variables explicatives modélisant les chocs de cette période.
La méthode la plus simple consiste à ajouter des indicatrices sur les trimestres concernés (ici l'année 2020 et le trimestre 2021T3)^[
D'autres spécifications pourraient être utilisées, comme par exemple l'ajout d'indicatrices sur l'ensemble des années 2020 et 2021 ou uniquement sur l'année 2020.
Ajouter ou retirer d'autres indicatrices peut sensiblement changer les résultats des coefficients estimés sur la fin de la période, notamment du fait du faible recul temporel que l'on a après le COVID-19 : le choix dépend aussi des hypothèses économiques que l'on fait sur le modèle utilisé (quels sont les trimestres qui sont atypiques et ceux dont les évolutions relèvent de la conjoncture ?).
Ici nous avons choisi de n'utiliser des indicatrices que pour l'année 2020 et le trimestre 2021T3 car les indicatrices des autres trimestres de 2021 ne sont pas significatives, que l'année 2020 est fortement heurtée par la crise du COVID-19 et que la forte croissance de 2021T3 peut s'expliquer par un contre-coup des mesures de confinement de 2021T2.
] :

```{r}
ind <- cbind(
  time(gdp) == 2020, time(gdp) == 2020.25, 
  time(gdp) == 2020.5, time(gdp) == 2020.75,
  time(gdp) == 2021.5
)
ind <- ts(apply(ind,2, as.numeric), start = start(gdp), frequency = 4)
colnames(ind) <- c(sprintf("ind2020T%i", 1:4), "ind2021T3")
data_covid <- ts.union(gdp, ind)
colnames(data_covid) <- c(colnames(gdp), colnames(ind))

reg_lin_covid <- dynlm(
  formula = growth_gdp ~ bc_fr_m1 + diff_bc_fr_m1 +
    ind2020T1 + ind2020T2 + ind2020T3 + ind2020T4 + 
    ind2021T3,
  data = window(data_covid, start = 1980)
)
summary(reg_lin_covid)
```
Pour la construction des autres modèles, nous gardons certains paramètres estimés en utilisant les données avant la période du COVID-19, cette dernière pouvant biaiser les résultats.
Ainsi\ :

- Pour la régression par morceaux, la date de rupture retenue est toujours 2000T3 (contre 2017T2 en utilisant les données après 2020).
```{r}
bp_covid <- breakpoints(reg_lin_covid)
c(breakdates(bp), breakdates(bp_covid))
```

- Pour la régression locale, la fenêtre utilisée est `{r} formatC(reg_loc$bw, digits = 2, decimal.mark = ",")` (proche de la fenêtre de 0,77 en utilisant les données après 2020).
```{r}
bw_covid <- bw(window(
  data_covid[, c("bc_fr_m1", "diff_bc_fr_m1",
                 "ind2020T1", "ind2020T2", 
                 "ind2020T3", "ind2020T4", 
                 "ind2021T3")], start = 1980, end = c(2023, 4)), 
  window(data_covid[, "growth_gdp"], start = 1980, end = c(2023, 4))
)
c(reg_loc$bw, bw_covid)
```

- Pour la régression avec coefficients stochastiques (modélisation espace-état), les coefficients associés aux indicatrices sont fixés (variance nulle, sinon ils sont considérés comme évoluant dans le temps) et le coefficient du climat des affaires en niveau est toujours considéré comme fixe.
Toutefois les variances des autres coefficients sont de nouveau estimées.
```{r}
ssm_covid <- ssm_lm(
  reg_lin_covid, 
  fixed_var_intercept = FALSE,
  fixed_var_variables = FALSE,
  var_intercept = 0.001,
  var_variables = 0.001
)
sqrt(ssm_covid$parameters$parameters * ssm_covid$parameters$scaling)
```

Les modèles sont donc estimés avec le code suivant :
```{r}
reg_morc_covid <- piece_reg(
  reg_lin_covid, break_dates = 2000.5,
  # Les indicatrices ne sont pas découpées
  fixed_var = 4:8)
reg_loc_covid <- tvReg::tvLM(
  formula = growth_gdp ~ bc_fr_m1 + diff_bc_fr_m1 +
    ind2020T1 + ind2020T2 + ind2020T3 + ind2020T4 + ind2021T3,
  data = window(data_covid, start = 1980),
  # On reprend l'ancienne fenêtre
  bw = reg_loc$bw 
)
ssm_covid <- ssm_lm(
  reg_lin_covid, 
  fixed_var_intercept = FALSE,
  # On fixe les coefficients des indicatrices
  # et le coefficient du climat des affaires en niveau 
  # (sinon il varie dans le temps)
  fixed_var_variables = c(c(TRUE, FALSE), rep(TRUE, 5)),
  var_intercept = 0.01,
  var_variables = c(0, 0.01, rep(0, 5))
)

coef_morc_covid <- coef(reg_morc_covid)
coef_lin_covid <- ts(matrix(coef(reg_lin_covid), nrow = 1), 
               start = start(coef_morc_covid),
               end = end(coef_morc_covid),
               frequency = frequency(coef_morc_covid))
colnames(coef_lin_covid) <- names(coef(reg_lin_covid))
coef_reg_loc_covid <- ts(coef(reg_loc_covid), start = 1980, frequency = 4)
coef_ssm_covid <- coef(ssm_covid)
```

La @fig-coef-covid montre les coefficients estimés en prenant en compte les données jusqu'au 2023T4.
L'analyse est similaire à celle de la @fig-coef-ssm sauf pour la régression avec coefficients stochastiques sur les dernières années : les variations du climat des affaires ayant un impact sur la prévision du PIB plus marqué après 2020, le coefficient associé est plus élevé après cette date mais aussi lors des années précédentes afin de lisser le passage à ce nouvel état de l'économie.
<!-- tu écris 2022T4 jusque-là, et ça passe à 2023T4 après. -->
<!-- Rep AQLT : j'ai mal mis à jour les dates lorsque j'ai actualisé les données, c'est bien 2023T4 -->
<!-- est-ce que ces résultats, en particulier la remontée très forte du coef sur diff(climat), est robuste à l’inclusion d’indicatrices sur les trimestres de 2021 aussi? 2021 est encore très chahutée. à voir si la référence que tu gardes c’est juste des indicatrices en 2020, ou alors aussi en 2021.-->
<!-- Rep AQLT : si on ajoute des indicatrices ça influence les résultats mais je ne suis pas sûr que cela signifie qu'il faille le faire. Pour le PIB, c'est surtout 2020 qui est impacté (sur la régression linéaire les indicatrices ne sont pas significatives en 2021 sauf au T3) et c'est aussi ce que fait la conj. -->

```{r}
#| echo: false
#| label: fig-coef-covid
#| fig-cap: "Coefficients estimés (hors indicatrices) par régression linéaire, régression par morceaux, régression locale (avec $b=0,75$) et régression avec coefficients stochastiques (modélisation espace-état) en utilisant les données jusqu'en 2023T4."

# res_is <- ts.union(ts(residuals(reg_lin_covid), start = c(1980, 1), frequency = 4),
#                    residuals(reg_morc_covid),
#                    ts(residuals(reg_loc_covid), start = c(1980, 1), frequency = 4),
#                    residuals(ssm_covid)[,"smoothed"]
# )
# apply(res_is, 2, rmse)
coef2plot(
  list(
    "Rég. linéaire" = coef_lin_covid,
    "Rég. par morceaux" = coef_morc_covid,
    "Rég. locale" = coef_reg_loc_covid,
    "Coef. stochastiques\n(espace-état)" = coef_ssm_covid
  )
)
```

Dans les données ici utilisées, le taux de croissance trimestriel du PIB est connu jusqu'au 2023T4 alors que les variables explicatives sont connues jusqu'au 2024T1 :
```{r}
tail(data_covid[, c("growth_gdp", "bc_fr_m1", "diff_bc_fr_m1")], 2)
```
Il est donc possible d'effectuer une prévision sur le dernier trimestre et nous allons maintenant montrer comment procéder.
Le plus simple est d'effectuer une somme pondérée des variables explicatives avec les coefficients estimés.
Dans cet exemple, les variables explicatives sont directement calculées dans la base de données en entrée et sont donc facile à extraire.
Lorsque ce n'est pas le cas (par exemple lorsque des variables retardées ou en différence sont directement calculées dans la formule de la fonction `dynlm()`^[
Ce qui aurait pu être le cas si le modèle avait été estimé en utilisant le paramètre `formula = growth_gdp ~ bc_fr_m1 + diff(bc_fr_m1, 1)` dans la fonction `dynlm()`.
]) il faut alors recalculer toutes les variables explicatives.
La fonction `tvCoef::full_exogeneous_matrix()` peut aider à effectuer cette tâche et ajoute également le régresseur associé à la constante (égal à 1) :
```{r}
data_prev <- full_exogeneous_matrix(reg_lin_covid)
# On extrait le dernier trimestre :
der_period <- tail(data_prev, 1)
der_period
# Transformation en numeric pour éviter des erreurs dues au format ts()
der_period <- as.numeric(der_period)
prev_reg_lin <- sum(coef(reg_lin_covid) * der_period)
# Pour les autres méthodes on prend les derniers coefficients estimés
prev_reg_morc <- sum(tail(coef(reg_morc_covid), 1) * der_period)
prev_reg_loc <- sum(tail(coef(reg_loc_covid), 1) * der_period)
prev_ssm <- sum(tail(coef(ssm_covid), 1) * der_period)
# Ensemble des prévisions pour 2023T1 :
round(
  c(prev_reg_lin, prev_reg_morc, prev_reg_loc, prev_ssm), 
  2)
```
Les prévisions entre les différentes méthodes sont proches car sur ce dernier trimestre le climat des affaires a peu évolué (différence de 0,4).


# Comparaison générale {#sec-comp-generales}

Dans cette section nous effectuons une comparaison plus détaillée des différentes méthodes utilisées.
Pour cela, nous utilisons 28 modèles de prévisions des taux de croissance trimestriels de l'industrie manufacturière et de ses principales sous-branches (voir annexe [-@sec-an-graph] pour les graphiques de ces variables).
Les modèles sont estimés entre 1990 et 2019 en utilisant des données issues des enquêtes de conjoncture de l'Insee et de la Banque de France, ainsi que l'Indice de Production Industrielle des branches étudiées.
Toutes les séries utilisées sont disponibles sous {{< fa brands r-project >}} dans la base de donnée `tvCoef::manufacturing`.
Parmi ces 28 modèles, la procédure de Bai et Perron détecte au moins une rupture sur 14 modèles et le test de Hansen, utilisé avec un seuil de 5 %, conclut à la présence de coefficients mobiles dans 5 modèles (@tbl-nb-models).
Cela permet donc également de comparer les résultats de la régression locale et de la régression avec coefficients stochastiques (modélisation espace-état) lorsque les coefficients ne sont pas considérés comme fixes par les tests étudiés.
Dans la suite, nous considérerons qu'un modèle n'a pas de rupture lorsque le test de Bai et Perron n'en détecte aucune : dans ce cas, la régression par morceaux donne le même résultat que la régression linéaire.

::: {#tbl-nb-models}
```{r}
#| echo: false
#| cache: false
read.csv2("img/nb_models.csv", check.names = FALSE) |> 
  gt(groupname_col = NULL,
     rowname_col = "Branche") |> 
  tab_spanner(columns = 3:4,
              "Avec rupture") |>
  grand_summary_rows(
    fns = list(
      Total ~ sum(.)
    )
  ) |> 
  cols_align(
    align = c("center")
  )  |> 
  cols_align(
    align = c("left"),
    columns = 1
  )
```
Lecture : dans la branche des biens d'équipement (C3), 6 modèles sont étudiés.
Dans 4 de ces modèles la procédure de Bai et Perron conclut à la présence d'au moins une rupture et le test joint d'Hansen conclut que les coefficients sont mobiles dans 2 de ces modèles.

Nombre de modèles étudiés par branche d'activité.
:::

Pour l'estimation des modèles de régression par morceaux, nous limitons le nombre maximal de ruptures à 2.
La dernière rupture est détectée en 2011T4, pour deux modèles de la branche des autres industries (C5).
Les prévisions hors échantillon sont calculées après 2013 afin d'éviter les fortes erreurs autour des ruptures.
Pour la régression locale et la régression avec coefficients stochastiques (modélisation espace-état), les modèles sont estimés avec les paramètres par défaut. 
Leur qualité prédictive pourrait même être améliorée avec une optimisation du modèle (par exemple en ne fixant pas les coefficients des indicatrices), au contraire de la régression linéaire ou par morceaux.
<!-- comme tu l’as formulé, on a envie de te demander de faire cette optimisation (on n’a pas le droit d’être fainéant dans un article, 28 modèles, ça se fait). Donc je propose une reformulation pour t’éviter ça. -->
<!-- Rep AQLT : en fait au début c'était fait, avec Claire on testait des modèles où l'on regardait quelles variables étaient fixées (selon le test de Hansen) et on les fixait. -->
<!-- Après réflexion, je me suis dit que faire quelque chose d'automatique n'était pas forcément la bonne démarche car il vaut mieux réfléchir à ce que l'on fait et aux hypothèses qu'il y a derrière.  -->
<!-- Avec cette façon de faire on regarde quelles sont les performances lorsque l'on applique ces méthodes sans réfléchir et sans "biaiser" les résultats par des choix qui peuvent être arbitraires. -->
<!-- Si tu penses qu'il peut être utile de rajouter une justification je peux le faire. -->

Pour comparer les prévisions, nous utilisons la racine carrée de l'erreur quadratique moyenne --- *Root-mean-square error* (RMSE).
Afin de comparer les résultats entre les différentes branches, nous normalisons les RMSE par celles calculées par la régression linéaire, c'est ce qui est montré dans la @tbl-error-table.
Dans l'ensemble ce sont les régressions avec coefficients stochastiques (modélisation espace-état) qui donnent les meilleurs résultats.
C’est d’abord le cas pour les modèles où une rupture a été détectée.
En effet dans l'échantillon, les performances sont proches entre les différentes méthodes (amélioration de la qualité prédictive moyenne d'environ 10 % par rapport à la régression linéaire).
Et hors échantillon, les résultats sont également améliorés avec la régression avec coefficients stochastiques pour la majorité des modèles (en moyenne de 5\ % et au maximum de 13\ %) mais sont dégradés pour 3 des 14 modèles (d'au plus 7\ %).
<!-- le premier 7% est un 5% non? -->
<!-- REP AQLT : j'ai du me mélanger entre des fichiers à jour ou non -->
<!-- je pense que tu as voulu donner trop de chiffres dans ce paragraphe (il y en a encore trop je pense, à la fin on n’en retient pas beaucoup du tout, alors que si tu en donnes juste quelques-uns, on les retiendra un peu mieux), et les arguments n’étaient pas vraiment mis dans un ordre logique, ce qui ne facilite pas la compréhension ni la mémorisation des résultats, j’ai modifié un peu, mais ça peut encore être amélioré. -->
<!-- Rep AQLT : le but n'était pas de retenir les chiffres mais plutôt d'appuyer/expliquer les propos mais je comprends ce que tu veux dire, j'ai supprimé certains chiffres -->
Alors que pour la régression locale, les résultats sont identiques ou dégradés dans la majorité des cas (9 modèles sur 14) ; et pour la régression par morceaux, même si pour 7 séries les résultats sont améliorés, cette amélioration semble moins forte qu'avec la régression avec coefficients stochastiques (au plus 9\ %).

<!-- vérifie les chiffres queje change -->
C’est ensuite, étonnamment, aussi le cas lorsqu'aucune rupture n'est détectée, puisque les régression avec coefficients stochastiques permettent également d'améliorer les résultats : dans l'échantillon les erreurs de prévision sont réduites d'en moyenne 9\ % et d'au plus 46\ % (contre 2 % en moyenne et d'au plus 15\ % pour la régression locale).
En temps réel elles sont améliorées pour huit modèles (d'au plus 15\ %) et ne sont que légèrement dégradées pour trois autres modèles.
Pour la régression locale, la performance hors échantillon est toujours dégradée.
Une partie de l'instabilité en temps réel provient du fait qu'aucune optimisation n'est faite dans la spécification des modèles.
Toutefois ces résultats suggèrent que les tests ici présentés pour tester la constance des coefficients ne sont pas toujours pertinents.
Ainsi, @abs2006 proposent une procédure de tests fondée sur la modélisation espace-état (en testant la significativité de la variance des coefficients estimés).

::: {#tbl-error-table}
```{r}
#| echo: false
#| cache: false
read.csv2("img/error_table.csv", check.names = FALSE) |> 
  gt(groupname_col = c("fixed", "Error")) |> 
  fmt_number(decimals = 2, dec_mark = ",", sep_mark = " ") |> 
  fmt_integer(
    columns = starts_with(c("<", "=", ">"))
  ) |> 
  tab_spanner(columns = starts_with(c("<", "=", ">")),
              "Séries dont RMSE") |> 
  cols_align(
    align = c("center")
  )  |> 
  cols_align(
    align = c("left"),
    columns = "rowname"
  )
```

Note : les modèles sans rupture sont ceux où aucune rupture n'est détectée par la procédure de Bai et Perron, la régression par morceaux coïncide alors avec la régression linéaire.

Lecture : pour les prévisions hors échantillon la régression avec coefficients stochastiques (modélisation espace-état) permet, par rapport à la régression linéaire, de réduire la RMSE d'en moyenne de 5\ % pour les modèles avec rupture et de 2\ % pour les modèles sans rupture.
Huit modèles sans rupture sont améliorés avec la régression avec coefficients stochastiques (avec une réduction maximale de la RMSE de 15\ %) et les erreurs de prévision sont augmentées pour trois modèles (avec une hausse maximale de 2\ %).

Racine carrée de l'erreur quadratique moyenne (RMSE) rapportée à celle des modèles régression linéaire.
:::


# Conclusion

En conclusion, cette étude montre comment, à partir d'un modèle de régression linéaire, l'hypothèse de constance des coefficients peut être testée et comment relâcher cette hypothèse en implémentant des modèles de régression par morceaux, de régression mobile et de régression avec coefficients stochastiques (modélisation espace-état).
Cette implémentation est facilitée grâce au package `tvCoef` (<https://github.com/InseeFrLab/tvCoef>) qui accompagne cette étude et tous les codes associés sont disponibles sous <https://github.com/InseeFrLab/DT-tvcoef>.

Lorsque les tests classiques indiquent une non-constance des coefficients, ces trois méthodes permettent de réduire les erreurs de prévision dans l'échantillon (lorsque les coefficients sont estimés sur l'ensemble des données).
Toutefois, ces trois méthodes reposent sur des hypothèses qui peuvent conduire à des fortes instabilités, notamment si elles sont utilisées naïvement lors des exercices de prévision en temps réel (hors échantillon).\
La régression par morceaux suppose la connaissance de dates de ruptures : même si des procédures existent pour leur détection automatique [@bai2003computation], les instabilités autour de celles-ci font qu'il est préférable de s'appuyer sur un raisonnement économique.
En effet, si rupture brutale il y a, elle doit pouvoir s'expliquer et le statisticien devrait pouvoir l'expliquer.\
Le paramètre principal de la régression locale est la fenêtre, qui permet de jouer sur la sensibilité des estimations aux observations lointaines.
Même s'il existe également des procédures de sélection automatique, leurs instabilités conduisent à des erreurs de prévision plus élevées que la régression linéaire lors des exercices de prévisions en temps réel.\
Enfin, dans les régressions avec coefficients stochastiques (modélisation espace-état), des instabilités numériques d'optimisation peuvent conduire à une volatilité dans l'estimation des variances des coefficients (qui déterminent si le coefficient varie ou non dans le temps et à quelle vitesse).
C'est toutefois la méthode qui donne les meilleurs résultats et qui permet dans la majorité des cas de réduire les erreurs de prévision par rapport à la régression linéaire, même lorsque les tests classiques indiquent une constance des coefficients !

Même si ces méthodes permettent, par rapport à la régression linéaire, d'améliorer la qualité des prévisions, elles n'ont pas vocation à remplacer les modèles existants mais plutôt à les compléter.
En effet, même si dans la majorité des cas les méthodes étudiées permettent de réduire les erreurs de prévision, cela peut ne pas être le cas sur tous les trimestres.
D'une part la combinaison de prévisions issues de différents modèles permet généralement d'obtenir une prévision finale plus précise
[voir par exemple @WANG20231518 pour une revue de littérature] ;
d'autre part, l'interprétation économique et les hypothèses sous-jacentes sont différentes entre chaque modèle : l'analyse faite de la prévision dépend donc également de la conjoncture récente.

Cette étude pourrait être étendue de plusieurs manières.
Tout d'abord, les méthodes ici présentées pourraient être améliorées.
Par exemple, pour la régression locale et la régression avec coefficients stochastiques, nous supposons que les paramètres évoluent à la même vitesse au cours de toute la période d'estimation (fenêtre fixe et variance fixée).
Toutefois, autour des périodes de crises (comme le COVID-19), il pourrait être pertinent d'ajouter plus de flexibilités à l'évolution des coefficients (en réduisant la fenêtre ou en effectuant un choc sur la variance) : cela ajouterait plus de variabilité dans les estimations mais pourrait permettre de mieux prendre en compte les changements structurels.\
Ensuite, d'autres méthodes d'estimations pourraient être utilisées pour modéliser l'évolution dans le temps des coefficients.
Par exemple @melard modélise des variations déterministes des coefficients (plutôt que stochastiques comme pour les modèles espace-état).\
Enfin, les modèles auraient également pu être comparés aux modèles à seuil et modèles à changement de régime markoviens [pour une revue bibliographique de ces modèles, voir par exemple @PETROPOULOS2022705] qui peuvent se voir comme des cas particuliers des méthodes étudiés.
En effet, dans les modèles à seuil la rupture est brutale et dépend du niveau d'une variable exogène et dans les modèles à changement de régime markoviens, les coefficients dépendent d'une variable inobservée modélisant la position de l'économie dans le cycle : la rupture est donc brutale et dépend d'une variable externe (comme dans la régression locale).
*In fine*, le choix entre toutes ces méthodes se fait surtout sur les hypothèses économiques que l'on souhaite modéliser. 



\newpage
\appendix

# Installation de `tvCoef` {.appendix}


Pour utiliser `tvCoef`, il faut il faut avoir la version 17 de Java SE (ou une version supérieure).

Pour savoir quelle version de Java est utilisée par R, utiliser le code suivant :
```{r}
#| eval: false
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
```

Si le résultat n'est pas sous la forme `"17xxxx"` c'est que vous n'avez pas Java 17 !

Si l'on a pas cette version d'installée et que l'on n'a pas les droits d'administrateur pour installer Java, une solution est d'installer une version portable de Java, par exemple installer une version portable à partir des liens suivants :

- [Zulu JDK](https://www.azul.com/downloads/#zulu)

- [AdoptOpenJDK](https://adoptopenjdk.net/)

- [Amazon Corretto](https://aws.amazon.com/corretto/)

Pour installer une version portable de java, télécharger par exemple le fichier `Windows 10 x64 Java Development Kit` disponible sur <https://jdk.java.net/java-se-ri/17>, le dézipper et le mettre par exemple sous `"D:/Programmes/jdk-17"`.  

Pour configurer R avec une version portable de Java, trois solutions\ :

1. Avant **tout chargement de package nécessitant Java (`rJava`...)** (si vous avez lancé le code précédent, relancez donc R) :
```{r, eval = FALSE}
# Si la version portable est installée sous D:/Programmes/jdk-17
Sys.setenv(JAVA_HOME='D:/Programmes/jdk-17')
```

2. Pour éviter de faire cette manipulation à chaque fois que l'on relance R, deux solutions\ :  

a. Modifier le `JAVA_HOME` dans les variables d'environnement de Windows (voir <https://confluence.atlassian.com/doc/setting-the-java_home-variable-in-windows-8895.html>).

b. Modifier le `.Renviron` : depuis R lancer le code `file.edit("~/.Renviron")`{.r}, ajouter dans le fichier le chemin vers la version portable de Java comme précédemment (`JAVA_HOME='D:/Programmes/jdk-17'`), sauvegarder et relancer R.

Il reste maintenant à installer les packages :

```{r, eval = FALSE}
# Nécessaire pour installer rjd3sts
remotes::install_github("rjdverse/rjd3toolkit")
# Pour installer rjd3sts (modèles espace-état)
remotes::install_github("rjdverse/rjd3sts")
remotes::install_github("InseeFrLab/tvCoef")
```

Si vous utilisez un ordinateur professionnel, si c'est nécessaire, pensez à configurer le proxy pour que ces commandes puissent fonctionner (voir <https://www.book.utilitr.org/01_r_insee/fiche-personnaliser-r#le-fichier-.renviron>).
Pour cela vous pouvez utiliser `curl::ie_get_proxy_for_url()` pour récupérer l'adresse du proxy et ajouter deux variable `http_proxy` et `https_proxy` dans les variables d'environnement (comme précédemment).

<!-- {{< pagebreak >}} -->

\newpage

# Annexe graphiques {#sec-an-graph .appendix}

```{r}
#| echo: false
graph_zoom <- function(x, start = 1980,
                       start_xlim= 1.75,
                       end_xlim = 1.7) {
  y <- window(x, start = start)
suppressMessages(autoplot(y) + theme_bw() +
  scale_x_continuous(breaks = seq.int(start, 2020, 5)) +
  facet_zoom(xlim = c(start + start_xlim, 2019.75 - end_xlim), shrink = F,
             ylim = range(window(y, end = 2019.75)),
             show.area = F,
             horizontal = F) +
  labs(y = NULL, x = NULL))
}
```


:::{#fig-graph-pib}
```{r}
#| echo: false
#| label: graph-pib
graph_zoom(gdp[,"growth_gdp"])
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).


Taux de croissance trimestriel du PIB français (variable `tvCoef::gdp[, "growth_gdp"]`).
:::

:::{#fig-graph-climatfrm1}
```{r}
#| echo: false
#| label: graph-climatfr-m1
graph_zoom(gdp[,"bc_fr_m1"])
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).


Climat des affaires France en niveau au premier mois de chaque trimestre (variable `tvCoef::gdp[, "bc_fr_m1"]`).
:::

:::{#fig-graph-diff-climatfrm1}
```{r}
#| echo: false
#| label: graph-diff-climatfr-m1
graph_zoom(gdp[,"diff_bc_fr_m1"])
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).


Différenciation trimestrielle du climat des affaires France au premier mois de chaque trimestre (variable `tvCoef::gdp[, "diff_bc_fr_m1"]`).
:::

:::{#fig-graph-manuf}
```{r}
#| echo: false
#| label: graph-manuf
graph_zoom(manufacturing[,"manuf_prod"], start = 1990, 1.4, 1.3)
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).


Taux de croissance trimestriel de la production manufacturière (variable `tvCoef::manufacturing[, "manuf_prod"]`).
:::

:::{#fig-graph-prodc1}
```{r}
#| echo: false
#| label: graph-prod-c1
graph_zoom(manufacturing[,"prod_c1"], start = 1990, 1.4, 1.3)
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).

Taux de croissance trimestriel de la production dans la branche agro–alimentaire (C1) (variable `tvCoef::manufacturing[, "prod_c1"]`).
:::

:::{#fig-graph-prodc3}
```{r}
#| echo: false
#| label: graph-prod-c3
graph_zoom(manufacturing[,"prod_c3"], start = 1990, 1.4, 1.3)
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).

Taux de croissance trimestriel de la production dans la branche biens d'équipement (C3) (variable `tvCoef::manufacturing[, "prod_c3"]`).
:::

:::{#fig-graph-prodc4}
```{r}
#| echo: false
#| label: graph-prod-c4
graph_zoom(manufacturing[,"prod_c4"], start = 1990, 1.4, 1.3)
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).

Taux de croissance trimestriel de la production dans la branche matériels de transport (C4) (variable `tvCoef::manufacturing[, "prod_c4"]`).
:::

:::{#fig-graph-prodc5}
```{r}
#| echo: false
#| label: graph-prod-c5
graph_zoom(manufacturing[,"prod_c5"], start = 1990, 1.4, 1.3)
```

Lecture : le premier graphique représente la série observée jusqu'au 2023T4 et le second graphique correspond à un zoom sur la période pre-covid (avant 2020).

Taux de croissance trimestriel de la production dans la branche autres industries (C5) (variable `tvCoef::manufacturing[, "prod_c5"]`).
:::

\newpage

# Bibliographie {-}

::: {#refs}
:::